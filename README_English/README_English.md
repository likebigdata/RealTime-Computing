# RealTime-Computing
Use real-time computing technology and web technology to build a big data Kanban to solve the problem. The practical technologies include mysql, Kafka, Flink, redis, flask and echarts

## Catalogue

[1.Problem demand](#1-Problem-demand)

[2.Scheme analysis](#2-Scheme-analysis)

[3.Installation environment](#3-Installation-environment)

[4.Environment start command and method of running code](#4-Environment-start-command-and-method-of-running-code)

[5.Description of code directory structure](#5-Description-of-code-directory-structure)

[6.Frequently asked questions](#6-Frequently-asked-questions)

[7.Author](#7-Author)


## 1. Problem demand

By simulating the real-time processing of order information of e-book platform, the massive order information is processed by using real-time computing related technology and distributed cluster idea. Finally, the data analysis results are displayed on the dashboard visualization screen.

-The interface is refreshed every 3 seconds. If it can be refreshed every 2 seconds or 1 second;
-Display the processed speed of the current order, with the unit of "piece / second";
-The big data Kanban interface shall be concise, atmospheric and characteristic;
-Display the basic information of total sales amount, order quantity and number of customers placed as of the current time;
-Display the ranking list of the top 10 books in sales as of the current time;
-Display the ranking list of the top 10 publishers in terms of sales as of the current time;
-Display the cumulative number of underground single customers in China (by province) and display it visually on the map;
-The data statistical error (data loss and statistical error) shall not exceed 1%, and experiments shall be designed to calculate the data error rate;
-The delay of displayed data shall not exceed 30 seconds, and the latest time of acquired data shall be displayed at each refresh;


## 2. Scheme analysis

The data generated by the order data simulator is stored in MySQL. By reading the binary log binlog of MySQL (recording all DDL and DML statements (except data query statements select and show), it is recorded in the form of events, including the time consumed by the execution of the statements. The binary log of MySQL is transaction safe.) Get the updated data of MySQL in real time, and then transmit it to Flink's datastream through Kafka. After extracting the main information from the stream data in datastream, change it into key value and store it in redis according to the reflection mechanism. Use Python's flask framework to obtain and process the data, and then send it to the defined front-end page (URL) through JSON data format, Finally, use the echarts framework to obtain the data in the corresponding URL of the requirements for large screen display of dashboard visual data.
![Scheme specific structure](/picture/Scheme_structure.png)


## 3. Installation environment

### Virtual machine environment

```
Scala	2.11.5
Kafka_2.11	2.4.1
Flink-bin-Scala_2.11	1.12.5
```

### Local environment

```
MySQL	8.0.27
Redis	4.0.14
```

#### [Native Python environment]

```
python	3.8
flask	1.1.2
flask-cors	3.0.10
```

### Import jar package locally

```
fasyjson	1.2.78
flink-connector-kafka_2.11	1.12.0
flink-connector-redis_2.11	1.0
jedis	2.9.0
commons-pool2	2.6.0
```

### A classified jar package folder is attached：（Import is also required）

```
Flink libs（All jar packages about Flink）
Kafka libs（All jar packages about Kafka）
Scala libs（All jar packages related to Scala）
MySQL-Kafka（All java files related to Kafka reading MySQL binlog logs）
```


## 4. Environment start command and method of running code

数据产生器 -> MysqlToKafka.java -> KafkaToFlink.java -> Flask.py

Specific startup steps:
```
    1).  Start MySQL database service and redis database service first
         (if you have run this project before, you need to use the "flush" command to clear all data in the redis database.)
    2).  To run the project, we also need to open zookeeper and Kafka distributed cluster framework in the virtual machine
    3).  First, reload the corresponding table in the MySQL database, set the number of threads, and start the data simulation generator.
    4).  Next, run mysqltokafka java、KafkaToFlink. Java and flask Py three programs. There are no hard requirements for their startup sequence, but here we suggest that we run the code program in the order of the overall implementation process: run mysqltokafka. Com first Java, and then run kafkatoflink Java, and finally run flash py。
    5).  After all the above programs are running, start the HTML page. Here, use the method in pychart that can directly open the corresponding HTML. Of course, you can also use the "127.0.0.1 /" IP page defined in the flash framework to open it.
```

## 5. Description of code directory structure

```
│  项目报告.docx
│  Dashboard实时大屏展示.mp4
│  README.txt
│  项目整体过程展示.mp4
│
├─README_English
│      README_English.txt
│  
├─作业项目要求
│      大作业要求.pdf
│      软件说明文档-v0.5.pdf
│
├─数据模拟生产器
│      allbook.csv
│      dist.zip
│      place.json
│      read.exe
│
├─相关配置文件
│  ├─本地导入jar包
│  │  │  commons-pool2-2.6.0.jar
│  │  │  fastjson-1.2.78.jar
│  │  │  flink-connector-kafka_2.11-1.12.0.jar
│  │  │  flink-connector-redis_2.11-1.0.jar
│  │  │  jedis-2.9.0.jar
│  │  │  
│  │  ├─Flink libs
│  │  │      
│  │  ├─Kafka libs
│  │  │      
│  │  ├─MySQL-Kafka
│  │  │  │  mysql-binlog-connector-java-master.zip
│  │  │  │  本目录说明和MySQl配置说明.txt
│  │  │  │  
│  │  │  └─com
│  │  │      └─github
│  │  │          └─shyiko
│  │  │              └─mysql
│  │  │                  └─binlog
│  │  │                                      
│  │  └─Scala libs
│  │          
│  ├─本地环境
│  │  │  本地相关python环境安装.txt
│  │  │  
│  │  ├─MySQL
│  │  │      mysql-8.0.27-winx64.msi
│  │  │      mysql-workbench-community-8.0.27-winx64.msi
│  │  │      
│  │  └─Redis
│  │          Redis 安装教程及使用命令.txt
│  │          Redis-x64-4.0.14.msi
│  │          
│  └─虚拟机环境
│          flink-1.12.5-bin-scala_2.11.tgz
│          flink安装教程.txt
│          kafka_2.11-2.4.1.tgz
│          kafka安装教程.txt
│          scala-2.11.5.tgz
│          scala安装教程.txt
│          
└─项目实现代码
    ├─Flask至可视化（Pycharm）
    │  │  Flask.py
    │  │  
    │  ├─static
    │  │  ├─css
    │  │  │      comon0.css
    │  │  │      index.html
    │  │  │      
    │  │  ├─font
    │  │  │      DS-DIGIT.TTF
    │  │  │      
    │  │  ├─images
    │  │  │      bg.jpg
    │  │  │      head_bg.png
    │  │  │      line(1).png
    │  │  │      
    │  │  ├─js
    │  │  │      area_echarts.js
    │  │  │      china.js
    │  │  │      echarts.min.js
    │  │  │      echarts1.js
    │  │  │      echarts2.js
    │  │  │      echarts3.js
    │  │  │      echarts4.js
    │  │  │      echarts5.js
    │  │  │      echarts6.js
    │  │  │      jquery.js
    │  │  │      
    │  │  └─picture
    │  │          jt.png
    │  │          lbx.png
    │  │          loading.gif
    │  │          map.png
    │  │          nchu.png
    │  │          weather.png
    │  │          
    │  └─templates
    │          Dashboard.html
    │          
    └─MySQL至Flink
            Java_Redis.java
            KafkaToFlink.java
            MysqlToKafka.java
            PaymentInfo.java
```


## 6. Frequently asked questions

1). Unable to connect to MySQL's binlog. After connecting to binlog, the corresponding data cannot be accurately proposed from the log.
```
Causes of the problem: one is that the relevant settings of binlog cannot be found, and the other is that the opening of user permissions in MySQL database is not comprehensive enough.
Solution: opening the binlog related settings file of MySQL is not accompanied by our installation location. If we need to find the location of the related file, we can query the location of the configuration file from MySQL through the related commands. We can also check whether we have opened all permissions for users through the relevant commands to view user permissions. Please pay attention to these configuration methods, because these jar packages are not provided through the normal official during the local connection of Java to MySQL binlog, so they may not be comprehensive. It will not prompt some errors encountered, but it is not connected, so that you have nowhere to find mistakes.
```
2). In Flink framework, data in datastream data stream cannot be extracted, classified and processed
```
Cause of problem: in this project experiment, the map method of datastream is used to extract and classify the data of datastream data stream. However, the method just touched the map can not fully use this method. Through official cases, we can only understand the usage of map for limited purposes, which is not applicable to the experimental requirements of this project.
Solution: after falling down many times and checking the class methods and official instructions in the original jar package many times, I slowly realized that using the map method allows users to customize methods suitable for users. It is similar to the map in the Scala language, or in other words, it is the map method in the Scala language.
```
3). Real time chart refresh problem (data cannot be refreshed in time, refresh screen, etc.)
```
Cause of the problem: at first, I didn't care about the difference of variables defined by VaR, let and const keywords. Let variables were defined in the for loop, so variables outside the for loop could not be used, and the data could not be updated in real time. After the data cannot be updated, try to use jQuery for local refresh, such as "$(" #echart1 "). Load (location. Href +" #echart1 ","); ", However, using this method will cause the code of all < script > tags to run again, resulting in page jamming and screen flashing.
Solution: finally, the method of local refresh using jQuery was abandoned, and the problems of let variable scope, no timer and only interval interval were found. Finally, the problem of real-time refresh was solved.
```
4). The front-end display process becomes stuck with the increase of data (using redis, memory storage occupation) over time
```
Cause of the problem: at first, I didn't know why the acquisition and processing of data can be completed at a high speed. Later, through the setting of test points one by one, I found that there are several points that need to be paid attention to, which may affect the Caton of the final visualization front end. One is that redis is based on memory storage. Can I allocate more memory to redis to solve the problem; The other is the millisecond delay in obtaining data from the front end F12. It is found that some data acquisition takes a lot of time, and the reason is not clear.
Solution: after the combined analysis of the last two suspected reasons, I feel that the two reasons affect each other. The main reason is redis memory storage, which leads to the slow operation of the overall local system. Therefore, we tried to change the storage mode, so that the process of using memory to store less key value data. In the end, we did solve the problem of changing the data on the original key. Although redis greatly improves the speed of query and storage, it also affects the operation efficiency of the whole machine in another way.
```
5). ......


## 7. Author

nono(Zechuan Wang)

Instructor：Fengyu Yang

Nanchang Hangkong University
